{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/blob/master/Chapter%208/Curiosity-Driven%20Exploration%20Super%20Mario.ipynbs\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "import time\n",
    "import gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the environment\n",
    "done = True\n",
    "for step in range(5000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "time.sleep(2)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Transformed Input Image')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def downscale_obs(obs, new_size=(42, 42), to_gray=True):\n",
    "    \"\"\"This function process input image so it has smaller size \n",
    "    and onnly one color channel \"\"\"\n",
    "    if to_gray:\n",
    "        return resize(obs, new_size, anti_aliasing=True).max(axis=2)\n",
    "    else:\n",
    "        return resize(obs, new_size, anti_aliasing=True)\n",
    "\n",
    "# plot transformed input image\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(121)\n",
    "img = env.reset()\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "gray_img = downscale_obs(img)\n",
    "plt.imshow(gray_img)\n",
    "plt.axis('off')\n",
    "plt.suptitle('Transformed Input Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the Q-Network \n",
    "class Qnetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.linear1 = nn.Linear(288,100)\n",
    "        self.linear2 = nn.Linear(100,12)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y))\n",
    "        y = y.flatten(start_dim=2)\n",
    "        y = y.view(y.shape[0], -1, 32)\n",
    "        y = y.flatten(start_dim=1)\n",
    "        y = F.elu(self.linear1(y))\n",
    "        y = self.linear2(y) \n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the encoder\n",
    "\n",
    "\n",
    "class Phi(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Phi, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.normalize(x)\n",
    "        y = F.elu(self.conv1(x))\n",
    "        y = F.elu(self.conv2(y))\n",
    "        y = F.elu(self.conv3(y))\n",
    "        y = F.elu(self.conv4(y)) \n",
    "        y = y.flatten(start_dim=1) \n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the inverse model\n",
    "class Gnet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Gnet, self).__init__()\n",
    "        self.linear1 = nn.Linear(576,256)\n",
    "        self.linear2 = nn.Linear(256,12)\n",
    "\n",
    "    def forward(self, state1,state2):\n",
    "        x = torch.cat( (state1, state2) ,dim=1)\n",
    "        y = F.relu(self.linear1(x))\n",
    "        y = self.linear2(y)\n",
    "        y = F.softmax(y,dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the forward model\n",
    "class Fnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Fnet, self).__init__()\n",
    "        self.linear1 = nn.Linear(300,256)\n",
    "        self.linear2 = nn.Linear(256,288)\n",
    "\n",
    "    def forward(self,state,action):\n",
    "        action_ = torch.zeros(action.shape[0],12)\n",
    "        indices = torch.stack( (torch.arange(action.shape[0]), action.squeeze()), dim=0)\n",
    "        indices = indices.tolist()\n",
    "        action_[indices] = 1.\n",
    "        x = torch.cat( (state,action_) ,dim=1)\n",
    "        y = F.relu(self.linear1(x))\n",
    "        y = self.linear2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the policy \n",
    "def policy(qvalues, eps=None): \n",
    "    \"\"\" Epsilon Greedy\"\"\"\n",
    "    if eps is not None:\n",
    "        if torch.rand(1) < eps:\n",
    "            return torch.randint(low=0,high=7,size=(1,))\n",
    "        else:\n",
    "            return torch.argmax(qvalues)\n",
    "    else:\n",
    "        return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experience replay\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, N=500, batch_size=100):\n",
    "        self.N = N \n",
    "        self.batch_size = batch_size\n",
    "        self.memory = [] \n",
    "        self.counter = 0\n",
    "        \n",
    "    def add_memory(self, state1, action, reward, state2):\n",
    "        self.counter +=1 \n",
    "        if self.counter % 500 == 0:\n",
    "            self.shuffle_memory()\n",
    "            \n",
    "        if len(self.memory) < self.N:\n",
    "            self.memory.append( (state1, action, reward, state2) )\n",
    "        else:\n",
    "            rand_index = np.random.randint(0,self.N-1)\n",
    "            self.memory[rand_index] = (state1, action, reward, state2) \n",
    "\n",
    "    def shuffle_memory(self):\n",
    "        shuffle(self.memory)\n",
    "        \n",
    "    def get_batch(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            batch_size = len(self.memory)\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        if len(self.memory) < 1:\n",
    "            print(\"Error: No data in memory.\")\n",
    "            return None\n",
    "\n",
    "        ind = np.random.choice(np.arange(len(self.memory)),batch_size,replace=False)\n",
    "        batch = [self.memory[i] for i in ind] #batch is a list of tuples\n",
    "        state1_batch = torch.stack([x[0].squeeze(dim=0) for x in batch],dim=0)\n",
    "        action_batch = torch.Tensor([x[1] for x in batch]).long()\n",
    "        reward_batch = torch.Tensor([x[2] for x in batch])\n",
    "        state2_batch = torch.stack([x[3].squeeze(dim=0) for x in batch],dim=0)\n",
    "        return state1_batch, action_batch, reward_batch, state2_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters\n",
    "params = {\n",
    "    'batch_size':150,\n",
    "    'beta':0.2,\n",
    "    'lambda':0.1,\n",
    "    'eta': 1.0,\n",
    "    'gamma':0.2,\n",
    "    'max_episode_len':100,\n",
    "    'min_progress':15,\n",
    "    'action_repeats':6,\n",
    "    'frames_per_state':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "replay = ExperienceReplay(N=1000, batch_size=params['batch_size'])\n",
    "Qmodel = Qnetwork()\n",
    "encoder = Phi()\n",
    "forward_model = Fnet()\n",
    "inverse_model = Gnet()\n",
    "forward_loss = nn.MSELoss(reduction='none')\n",
    "inverse_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "qloss = nn.MSELoss()\n",
    "all_model_params = list(Qmodel.parameters()) + list(encoder.parameters()) \n",
    "all_model_params += list(forward_model.parameters()) + list(inverse_model.parameters())\n",
    "opt = optim.Adam(lr=0.001, params=all_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICM(state1, action, state2, forward_scale=1., inverse_scale=1e4): #action is an integer [0:11]\n",
    "    \"\"\" Intrinsic Curiosity Module (ICM): Calculates prediction error for forward and inverse dynamics \"\"\"\n",
    "    state1_hat = encoder(state1)\n",
    "    state2_hat = encoder(state2)\n",
    "    state2_hat_pred = forward_model(state1_hat.detach(), action.detach())\n",
    "    forward_pred_err = forward_scale * forward_loss(state2_hat_pred, \\\n",
    "                        state2_hat.detach()).sum(dim=1).unsqueeze(dim=1)\n",
    "    pred_action = inverse_model(state1_hat, state2_hat) \n",
    "    inverse_pred_err = inverse_scale * inverse_loss(pred_action, \\\n",
    "                                        action.detach().flatten()).unsqueeze(dim=1)\n",
    "    return forward_pred_err, inverse_pred_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(q_loss, inverse_loss, forward_loss):\n",
    "    \"\"\"\n",
    "    Overall loss function to optimize for all 4 modules\n",
    "    Loss function based on calculation in paper\n",
    "    \"\"\"\n",
    "    loss_ = (1 - params['beta']) * inverse_loss\n",
    "    loss_ += params['beta'] * forward_loss\n",
    "    loss_ = loss_.sum() / loss_.flatten().shape[0]\n",
    "    loss = loss_ + params['lambda'] * q_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_state(state):\n",
    "    \"\"\"\n",
    "    First downscale state, convert to grayscale, convert to torch tensor and add batch dimension\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(downscale_obs(state, to_gray=True)).float().unsqueeze(dim=0)\n",
    "\n",
    "def prepare_multi_state(state1, state2):\n",
    "    \"\"\"\n",
    "    Prepare a 3 channel state (for use in inference not training).\n",
    "    \n",
    "    The Q-model and encoder/Phi model expect the input state to have 3 channels. Following the reference paper,\n",
    "    these models are fed 3 consecutive state frames to give the model's access to motion information \n",
    "    (i.e. velocity information rather than just positional information)\n",
    "    \"\"\"\n",
    "    #prev is 1x3x42x42\n",
    "    state1 = state1.clone()\n",
    "    tmp = torch.from_numpy(downscale_obs(state2, to_gray=True)).float()\n",
    "    #shift data along tensor to accomodate newest observation (we could have used deque w/ maxlen 3)\n",
    "    state1[0][0] = state1[0][1]\n",
    "    state1[0][1] = state1[0][2]\n",
    "    state1[0][2] = tmp #replace last frame\n",
    "    return state1\n",
    "\n",
    "def prepare_initial_state(state,N=3):\n",
    "    \"\"\"\n",
    "    Prepares the initial state which is just a tensor of 1 (Batch) x 3 x 42 x 42\n",
    "    \n",
    "    The channel dimension is just a copy of the input state 3 times\n",
    "    \n",
    "    \"\"\"\n",
    "    #state should be 42x42 array\n",
    "    state_ = torch.from_numpy(downscale_obs(state, to_gray=True)).float()\n",
    "    tmp = state_.repeat((N,1,1)) #now 3x42x42\n",
    "    return tmp.unsqueeze(dim=0) #now 1x3x42x42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env():\n",
    "    \"\"\"\n",
    "    Reset the environment and return a new initial state\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    state1 = prepare_initial_state(env.render('rgb_array'))\n",
    "    return state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_train(use_extrinsic=True):\n",
    "    state1_batch, action_batch, reward_batch, state2_batch = replay.get_batch() \n",
    "    action_batch = action_batch.view(action_batch.shape[0],1)\n",
    "    reward_batch = reward_batch.view(reward_batch.shape[0],1)\n",
    "    #replay.get_batch returns tuple (state1, action, reward, state2) where each tensor has batch dimension\n",
    "    forward_pred_err, inverse_pred_err = ICM(state1_batch, action_batch, state2_batch) #internal curiosity module\n",
    "    i_reward = (1. / params['eta']) * forward_pred_err\n",
    "    reward = i_reward.detach()\n",
    "    if use_extrinsic:\n",
    "        reward += reward_batch \n",
    "    qvals = Qmodel(state2_batch)\n",
    "    reward += params['gamma'] * torch.max(qvals)\n",
    "    reward_pred = Qmodel(state1_batch)\n",
    "    reward_target = reward_pred.clone()\n",
    "    indices = torch.stack( (torch.arange(action_batch.shape[0]), action_batch.squeeze()), dim=0)\n",
    "    indices = indices.tolist()\n",
    "    reward_target[indices] = reward.squeeze()\n",
    "    q_loss = 1e5 * qloss(F.normalize(reward_pred), F.normalize(reward_target.detach()))\n",
    "    return forward_pred_err, inverse_pred_err, q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arturo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/arturo/anaconda3/lib/python3.7/site-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250, Loss: 4900.20947265625\n",
      "Forward loss: 1.5566306114196777 \n",
      " Inverse loss: 24483.7734375 \n",
      " Qloss: 22.084510803222656\n",
      "{'coins': 2, 'flag_get': False, 'life': 1, 'score': 700, 'stage': 1, 'status': 'small', 'time': 391, 'world': 1, 'x_pos': 8, 'x_pos_screen': 8, 'y_pos': 117}\n",
      "Episode over.\n",
      "Episode over.\n",
      "Epoch 500, Loss: 5010.20703125\n",
      "Forward loss: 0.06665535271167755 \n",
      " Inverse loss: 24879.328125 \n",
      " Qloss: 342.8762512207031\n",
      "{'coins': 0, 'flag_get': False, 'life': 1, 'score': 0, 'stage': 1, 'status': 'small', 'time': 383, 'world': 1, 'x_pos': 13, 'x_pos_screen': 13, 'y_pos': 122}\n",
      "Episode over.\n",
      "Episode over.\n",
      "Epoch 750, Loss: 4990.48193359375\n",
      "Forward loss: 0.09179939329624176 \n",
      " Inverse loss: 24792.94921875 \n",
      " Qloss: 318.19342041015625\n",
      "{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 385, 'world': 1, 'x_pos': 43, 'x_pos_screen': 43, 'y_pos': 93}\n",
      "Epoch 1000, Loss: 5005.41650390625\n",
      "Forward loss: 0.031126737594604492 \n",
      " Inverse loss: 24842.095703125 \n",
      " Qloss: 369.7134094238281\n",
      "{'coins': 3, 'flag_get': False, 'life': 1, 'score': 600, 'stage': 1, 'status': 'small', 'time': 332, 'world': 1, 'x_pos': 404, 'x_pos_screen': 80, 'y_pos': 82}\n",
      "Episode over.\n",
      "Epoch 1250, Loss: 5002.701171875\n",
      "Forward loss: 0.14860722422599792 \n",
      " Inverse loss: 24843.958984375 \n",
      " Qloss: 337.9006652832031\n",
      "{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 385, 'world': 1, 'x_pos': 0, 'x_pos_screen': 0, 'y_pos': 79}\n",
      "Episode over.\n",
      "Episode over.\n",
      "Episode over.\n",
      "Epoch 1500, Loss: 5099.55224609375\n",
      "Forward loss: 10.479836463928223 \n",
      " Inverse loss: 24580.892578125 \n",
      " Qloss: 1749.90283203125\n",
      "{'coins': 1, 'flag_get': False, 'life': 0, 'score': 200, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 41, 'x_pos_screen': 41, 'y_pos': 79}\n",
      "Episode over.\n",
      "Episode over.\n"
     ]
    }
   ],
   "source": [
    "epochs = 2500\n",
    "env.reset()\n",
    "state1 = prepare_initial_state(env.render('rgb_array'))\n",
    "eps=0.15\n",
    "losses = []\n",
    "ep_lengths = []\n",
    "episode_length = 0\n",
    "switch_to_eps_greedy = 1000\n",
    "state_deque = deque(maxlen=params['frames_per_state'])\n",
    "e_reward = 0.\n",
    "last_x_pos = env.env.env._x_position\n",
    "for i in range(epochs):\n",
    "    opt.zero_grad()\n",
    "    episode_length += 1\n",
    "    q_val_pred = Qmodel(state1)\n",
    "    if i > switch_to_eps_greedy:\n",
    "        action = int(policy(q_val_pred,eps))\n",
    "    else:\n",
    "        action = int(policy(q_val_pred))\n",
    "    for j in range(params['action_repeats']):\n",
    "        state2, e_reward_, done, info = env.step(action)\n",
    "        if done:\n",
    "            state1 = reset_env()\n",
    "            break\n",
    "        e_reward += e_reward_\n",
    "        state_deque.append(prepare_state(state2))\n",
    "    state2 = torch.stack(list(state_deque),dim=1)\n",
    "    replay.add_memory(state1, action, e_reward, state2)\n",
    "    e_reward = 0\n",
    "    if i % params['max_episode_len'] == 0 and i != 0:\n",
    "        if (info['x_pos'] - last_x_pos) < params['min_progress']:\n",
    "            done = True\n",
    "        else:\n",
    "            last_x_pos = info['x_pos']\n",
    "    if done:\n",
    "        print(\"Episode over.\")\n",
    "        ep_lengths.append(info['x_pos'])\n",
    "        state1 = reset_env()\n",
    "        last_x_pos = env.env.env._x_position\n",
    "        episode_length = 0\n",
    "    else:\n",
    "        state1 = state2\n",
    "    #Enter mini-batch training\n",
    "    if len(replay.memory) < params['batch_size']:\n",
    "        continue\n",
    "    \n",
    "    forward_pred_err, inverse_pred_err, q_loss = minibatch_train(use_extrinsic=False)\n",
    "    loss = loss_fn(q_loss, forward_pred_err, inverse_pred_err)\n",
    "    loss_list = (q_loss.mean(), forward_pred_err.flatten().mean(), inverse_pred_err.flatten().mean(), episode_length)\n",
    "    if i % 250 == 0:\n",
    "        print(\"Epoch {}, Loss: {}\".format(i,loss))\n",
    "        print(\"Forward loss: {} \\n Inverse loss: {} \\n Qloss: {}\".format(\\\n",
    "                             forward_pred_err.mean(),inverse_pred_err.mean(),q_loss.mean()))\n",
    "        print(info)\n",
    "    losses.append(loss_list)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_ = np.array(losses)\n",
    "ep_lengths_ = np.array(ep_lengths)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.log(losses_[:,0]),label='Q loss')\n",
    "plt.plot(np.log(losses_[:,1]),label='Forward loss')\n",
    "plt.plot(np.log(losses_[:,2]),label='Inverse loss')\n",
    "#plt.plot(ep_lengths_, label='Episode Length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "eps=0.15\n",
    "done = True\n",
    "state_deque = deque(maxlen=params['frames_per_state'])\n",
    "for step in range(5000):\n",
    "    if done:\n",
    "        env.reset()\n",
    "        state1 = prepare_initial_state(env.render('rgb_array'))\n",
    "    q_val_pred = Qmodel(state1)\n",
    "    action = int(policy(q_val_pred,eps))\n",
    "    state2, reward, done, info = env.step(action)\n",
    "    state2 = prepare_multi_state(state1,state2)\n",
    "    state1=state2\n",
    "    env.render()\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(q,tau=1.4): #q is vector\n",
    "    q = F.normalize(q)\n",
    "    return torch.exp(q/tau) / torch.sum(torch.exp(q/tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoder(from_replay=False):\n",
    "    \"\"\"\n",
    "    Test's the encoder's ability to disentangle similar states.\n",
    "    \n",
    "    If the encoder is being properly trained, it should learn to encode similar states such that their \n",
    "    euclidian distance is relatively large, so that the the forward network and inverse network can make\n",
    "    better predictions. You'll notice that if you run the function before or early during training \n",
    "    (make sure from_replay is false since the replay will be empty before training) the euclidian distance\n",
    "    between two states will be small, but during training the encoder will learn to disentangle these and the \n",
    "    distance will increase.\n",
    "    \n",
    "    `from_replay=True` will test 2 consecutive states from the replay memory otherwise will just reset environment\n",
    "    and use initial two states after taking action\n",
    "    \"\"\"\n",
    "    if from_replay:\n",
    "        assert len(replay.memory) > 0\n",
    "        s1, a, r, s2 = replay.memory[np.random.randint(len(replay.memory))]\n",
    "    else:\n",
    "        env.reset()\n",
    "        s1 = prepare_initial_state(env.render('rgb_array'))\n",
    "        env.reset()\n",
    "        env.step(3)\n",
    "        s2 = prepare_multi_state(s1, env.render('rgb_array'))\n",
    "        \n",
    "    return nn.MSELoss(reduction='mean')(encoder(s1),encoder(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model parameters\n",
    "torch.save(Qmodel.state_dict(),'Qmodel_')\n",
    "torch.save(encoder.state_dict(),'encoder_')\n",
    "torch.save(forward_model.state_dict(),'Fnet_')\n",
    "torch.save(inverse_model.state_dict(),'Gnet_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
