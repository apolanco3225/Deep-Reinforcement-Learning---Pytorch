{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "curiosity_driven_agent.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMx9sDTGTKV+kz0GQIFZ7AK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apolanco3225/Deep-Reinforcement-Learning---Pytorch/blob/master/curiosity_driven_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AnqpfDPxKIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "from collections import deque\n",
        "from IPython import display\n",
        "\n",
        "import gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYeBAJy7ylmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "done = True\n",
        "for step in range(5000):\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "    env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8zIz-FT4vIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def downscale_obs(obs, new_size=(42,42), to_gray=True):\n",
        "    \"\"\"\n",
        "    downscale_obs: rescales RGB image to lower dimensions with option to change to grayscale\n",
        "    \n",
        "    obs: Numpy array or PyTorch Tensor of dimensions Ht x Wt x 3 (channels)\n",
        "    \n",
        "    to_gray: if True, will use max to sum along channel dimension for greatest contrast\n",
        "    \"\"\"\n",
        "    if to_gray:\n",
        "        return resize(obs, new_size, anti_aliasing=True).max(axis=2)\n",
        "    else:\n",
        "        return resize(obs, new_size, anti_aliasing=True)\n",
        "\n",
        "\n",
        "class Qnetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Qnetwork, self).__init__()\n",
        "        #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
        "        self.linear1 = nn.Linear(288,100)\n",
        "        self.linear2 = nn.Linear(100,12)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = F.normalize(x)\n",
        "        y = F.elu(self.conv1(x))\n",
        "        y = F.elu(self.conv2(y))\n",
        "        y = F.elu(self.conv3(y))\n",
        "        y = F.elu(self.conv4(y))\n",
        "        y = y.flatten(start_dim=2)\n",
        "        y = y.view(y.shape[0], -1, 32)\n",
        "        y = y.flatten(start_dim=1)\n",
        "        y = F.elu(self.linear1(y))\n",
        "        y = self.linear2(y) #size N, 12\n",
        "        return y\n",
        "\n",
        "\n",
        "class Phi(nn.Module): # (raw state) -> low dim state\n",
        "    def __init__(self):\n",
        "        super(Phi, self).__init__()\n",
        "        #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=2, padding=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = F.normalize(x)\n",
        "        y = F.elu(self.conv1(x))\n",
        "        y = F.elu(self.conv2(y))\n",
        "        y = F.elu(self.conv3(y))\n",
        "        y = F.elu(self.conv4(y)) #size [1, 32, 3, 3] batch, channels, 3 x 3\n",
        "        y = y.flatten(start_dim=1) #size N, 288\n",
        "        return y\n",
        "\n",
        "class Gnet(nn.Module): #Inverse model: (phi_state1, phi_state2) -> action\n",
        "    def __init__(self):\n",
        "        super(Gnet, self).__init__()\n",
        "        #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "        self.linear1 = nn.Linear(576,256)\n",
        "        self.linear2 = nn.Linear(256,12)\n",
        "\n",
        "    def forward(self, state1,state2):\n",
        "        x = torch.cat( (state1, state2) ,dim=1)\n",
        "        y = F.relu(self.linear1(x))\n",
        "        y = self.linear2(y)\n",
        "        y = F.softmax(y,dim=1)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class Fnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Fnet, self).__init__()\n",
        "        #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "        self.linear1 = nn.Linear(300,256)\n",
        "        self.linear2 = nn.Linear(256,288)\n",
        "\n",
        "    def forward(self,state,action):\n",
        "        action_ = torch.zeros(action.shape[0],12)\n",
        "        indices = torch.stack( (torch.arange(action.shape[0]), action.squeeze()), dim=0)\n",
        "        indices = indices.tolist()\n",
        "        action_[indices] = 1.\n",
        "        x = torch.cat( (state,action_) ,dim=1)\n",
        "        y = F.relu(self.linear1(x))\n",
        "        y = self.linear2(y)\n",
        "        return y\n",
        "def policy(qvalues, eps=None): #Epsilon greedy\n",
        "    \"\"\"\n",
        "    policy(qvales, eps=None) takes Q-values and produces an integer representing action\n",
        "    \n",
        "    The function takes a vector of dimension (12,) representing Q-values for each of the 12 discrete actions\n",
        "    and returns an integer. If `eps` is supplied it follows an epsilon-greedy policy with probability `eps`. If `eps`\n",
        "    is not supplied, then a softmax policy is used.\n",
        "    \n",
        "    \"\"\"\n",
        "    if eps is not None:\n",
        "        if torch.rand(1) < eps:\n",
        "            return torch.randint(low=0,high=7,size=(1,))\n",
        "        else:\n",
        "            return torch.argmax(qvalues)\n",
        "    else:\n",
        "        return torch.multinomial(F.softmax(F.normalize(qvalues)), num_samples=1)\n",
        "\n",
        "class ExperienceReplay:\n",
        "    def __init__(self, N=500, batch_size=100):\n",
        "        self.N = N #total memory size\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = [] #list of tuples of tensors (S_t, a_t, R_{t+1}, S_{t+1})\n",
        "        self.counter = 0\n",
        "        #S_t should be size B x Channel x Ht x Wt. R_t : B x 1\n",
        "        \n",
        "    def add_memory(self, state1, action, reward, state2):\n",
        "        self.counter +=1 \n",
        "        if self.counter % 500 == 0:\n",
        "            self.shuffle_memory()\n",
        "            \n",
        "        if len(self.memory) < self.N:\n",
        "            self.memory.append( (state1, action, reward, state2) )\n",
        "        else:\n",
        "            rand_index = np.random.randint(0,self.N-1)\n",
        "            self.memory[rand_index] = (state1, action, reward, state2) #replace random memory\n",
        "    \n",
        "    def shuffle_memory(self):\n",
        "        shuffle(self.memory)\n",
        "        \n",
        "    def get_batch(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            batch_size = len(self.memory)\n",
        "        else:\n",
        "            batch_size = self.batch_size\n",
        "        if len(self.memory) < 1:\n",
        "            print(\"Error: No data in memory.\")\n",
        "            return None\n",
        "\n",
        "        ind = np.random.choice(np.arange(len(self.memory)),batch_size,replace=False)\n",
        "        batch = [self.memory[i] for i in ind] #batch is a list of tuples\n",
        "        state1_batch = torch.stack([x[0].squeeze(dim=0) for x in batch],dim=0)\n",
        "        action_batch = torch.Tensor([x[1] for x in batch]).long()\n",
        "        reward_batch = torch.Tensor([x[2] for x in batch])\n",
        "        state2_batch = torch.stack([x[3].squeeze(dim=0) for x in batch],dim=0)\n",
        "        return state1_batch, action_batch, reward_batch, state2_batch\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juIWHAWt4zYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "params = {\n",
        "    'batch_size':150,\n",
        "    'beta':0.2,\n",
        "    'lambda':0.1,\n",
        "    'eta': 1.0,\n",
        "    'gamma':0.2,\n",
        "    'max_episode_len':100,\n",
        "    'min_progress':15,\n",
        "    'action_repeats':6,\n",
        "    'frames_per_state':3\n",
        "}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2PR3CvM40HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay = ExperienceReplay(N=1000, batch_size=params['batch_size'])\n",
        "Qmodel = Qnetwork()\n",
        "encoder = Phi()\n",
        "forward_model = Fnet()\n",
        "inverse_model = Gnet()\n",
        "forward_loss = nn.MSELoss(reduction='none')#torch.nn.PairwiseDistance()#\n",
        "inverse_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "qloss = nn.MSELoss()\n",
        "# We can add the model parameters from each model to a list and pass that to a single optimizer\n",
        "all_model_params = list(Qmodel.parameters()) + list(encoder.parameters()) \n",
        "all_model_params += list(forward_model.parameters()) + list(inverse_model.parameters())\n",
        "opt = optim.Adam(lr=0.001, params=all_model_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psBuB0eM46uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def ICM(state1, action, state2, forward_scale=1., inverse_scale=1e4): #action is an integer [0:11]\n",
        "    \"\"\"\n",
        "    Intrinsic Curiosity Module (ICM): Calculates prediction error for forward and inverse dynamics\n",
        "    \n",
        "    The ICM takes a state1, the action that was taken, and the resulting state2 as inputs\n",
        "    (from experience replay memory) and uses the forward and inverse models to calculate the prediction error\n",
        "    and train the encoder to only pay attention to details in the environment that are controll-able (i.e. it should\n",
        "    learn to ignore useless stochasticity in the environment and not encode that).\n",
        "    \n",
        "    \"\"\"\n",
        "    state1_hat = encoder(state1)\n",
        "    state2_hat = encoder(state2)\n",
        "    #Forward model prediction error\n",
        "    state2_hat_pred = forward_model(state1_hat.detach(), action.detach())\n",
        "    forward_pred_err = forward_scale * forward_loss(state2_hat_pred, \\\n",
        "                        state2_hat.detach()).sum(dim=1).unsqueeze(dim=1)\n",
        "    #Inverse model prediction error\n",
        "    pred_action = inverse_model(state1_hat, state2_hat) #returns softmax over actions\n",
        "    inverse_pred_err = inverse_scale * inverse_loss(pred_action, \\\n",
        "                                        action.detach().flatten()).unsqueeze(dim=1)\n",
        "    return forward_pred_err, inverse_pred_err\n",
        "\n",
        "\n",
        "def loss_fn(q_loss, inverse_loss, forward_loss):\n",
        "    \"\"\"\n",
        "    Overall loss function to optimize for all 4 modules\n",
        "    \n",
        "    Loss function based on calculation in paper\n",
        "    \"\"\"\n",
        "    loss_ = (1 - params['beta']) * inverse_loss\n",
        "    loss_ += params['beta'] * forward_loss\n",
        "    loss_ = loss_.sum() / loss_.flatten().shape[0]\n",
        "    loss = loss_ + params['lambda'] * q_loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "def prepare_state(state):\n",
        "    \"\"\"\n",
        "    First downscale state, convert to grayscale, convert to torch tensor and add batch dimension\n",
        "    \"\"\"\n",
        "    return torch.from_numpy(downscale_obs(state, to_gray=True)).float().unsqueeze(dim=0)\n",
        "\n",
        "def prepare_multi_state(state1, state2):\n",
        "    \"\"\"\n",
        "    Prepare a 3 channel state (for use in inference not training).\n",
        "    \n",
        "    The Q-model and encoder/Phi model expect the input state to have 3 channels. Following the reference paper,\n",
        "    these models are fed 3 consecutive state frames to give the model's access to motion information \n",
        "    (i.e. velocity information rather than just positional information)\n",
        "    \"\"\"\n",
        "    #prev is 1x3x42x42\n",
        "    state1 = state1.clone()\n",
        "    tmp = torch.from_numpy(downscale_obs(state2, to_gray=True)).float()\n",
        "    #shift data along tensor to accomodate newest observation (we could have used deque w/ maxlen 3)\n",
        "    state1[0][0] = state1[0][1]\n",
        "    state1[0][1] = state1[0][2]\n",
        "    state1[0][2] = tmp #replace last frame\n",
        "    return state1\n",
        "\n",
        "def prepare_initial_state(state,N=3):\n",
        "    \"\"\"\n",
        "    Prepares the initial state which is just a tensor of 1 (Batch) x 3 x 42 x 42\n",
        "    \n",
        "    The channel dimension is just a copy of the input state 3 times\n",
        "    \n",
        "    \"\"\"\n",
        "    #state should be 42x42 array\n",
        "    state_ = torch.from_numpy(downscale_obs(state, to_gray=True)).float()\n",
        "    tmp = state_.repeat((N,1,1)) #now 3x42x42\n",
        "    return tmp.unsqueeze(dim=0) #now 1x3x42x42\n",
        "\n",
        ":\n",
        "\n",
        "def reset_env():\n",
        "    \"\"\"\n",
        "    Reset the environment and return a new initial state\n",
        "    \"\"\"\n",
        "    env.reset()\n",
        "    state1 = prepare_initial_state(env.render('rgb_array'))\n",
        "    return state1\n",
        "\n",
        "\n",
        "\n",
        "def minibatch_train(use_extrinsic=True):\n",
        "    state1_batch, action_batch, reward_batch, state2_batch = replay.get_batch() \n",
        "    action_batch = action_batch.view(action_batch.shape[0],1)\n",
        "    reward_batch = reward_batch.view(reward_batch.shape[0],1)\n",
        "    #replay.get_batch returns tuple (state1, action, reward, state2) where each tensor has batch dimension\n",
        "    forward_pred_err, inverse_pred_err = ICM(state1_batch, action_batch, state2_batch) #internal curiosity module\n",
        "    i_reward = (1. / params['eta']) * forward_pred_err\n",
        "    reward = i_reward.detach()\n",
        "    if use_extrinsic:\n",
        "        reward += reward_batch \n",
        "    qvals = Qmodel(state2_batch)\n",
        "    reward += params['gamma'] * torch.max(qvals)\n",
        "    reward_pred = Qmodel(state1_batch)\n",
        "    reward_target = reward_pred.clone()\n",
        "    indices = torch.stack( (torch.arange(action_batch.shape[0]), action_batch.squeeze()), dim=0)\n",
        "    indices = indices.tolist()\n",
        "    reward_target[indices] = reward.squeeze()\n",
        "    q_loss = 1e5 * qloss(F.normalize(reward_pred), F.normalize(reward_target.detach()))\n",
        "    return forward_pred_err, inverse_pred_err, q_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oNIk_O449tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 2500\n",
        "env.reset()\n",
        "state1 = prepare_initial_state(env.render('rgb_array'))\n",
        "eps=0.15\n",
        "losses = []\n",
        "ep_lengths = []\n",
        "episode_length = 0\n",
        "switch_to_eps_greedy = 1000\n",
        "state_deque = deque(maxlen=params['frames_per_state'])\n",
        "e_reward = 0.\n",
        "last_x_pos = env.env.env._x_position\n",
        "for i in range(epochs):\n",
        "    opt.zero_grad()\n",
        "    episode_length += 1\n",
        "    q_val_pred = Qmodel(state1)\n",
        "    if i > switch_to_eps_greedy:\n",
        "        action = int(policy(q_val_pred,eps))\n",
        "    else:\n",
        "        action = int(policy(q_val_pred))\n",
        "    for j in range(params['action_repeats']):\n",
        "        state2, e_reward_, done, info = env.step(action)\n",
        "        if done:\n",
        "            state1 = reset_env()\n",
        "            break\n",
        "        e_reward += e_reward_\n",
        "        state_deque.append(prepare_state(state2))\n",
        "    state2 = torch.stack(list(state_deque),dim=1)\n",
        "    replay.add_memory(state1, action, e_reward, state2)\n",
        "    e_reward = 0\n",
        "    if i % params['max_episode_len'] == 0 and i != 0:\n",
        "        if (info['x_pos'] - last_x_pos) < params['min_progress']:\n",
        "            done = True\n",
        "        else:\n",
        "            last_x_pos = info['x_pos']\n",
        "    if done:\n",
        "        print(\"Episode over.\")\n",
        "        ep_lengths.append(info['x_pos'])\n",
        "        state1 = reset_env()\n",
        "        last_x_pos = env.env.env._x_position\n",
        "        episode_length = 0\n",
        "    else:\n",
        "        state1 = state2\n",
        "    #Enter mini-batch training\n",
        "    if len(replay.memory) < params['batch_size']:\n",
        "        continue\n",
        "    \n",
        "    forward_pred_err, inverse_pred_err, q_loss = minibatch_train(use_extrinsic=False)\n",
        "    loss = loss_fn(q_loss, forward_pred_err, inverse_pred_err)\n",
        "    loss_list = (q_loss.mean(), forward_pred_err.flatten().mean(), inverse_pred_err.flatten().mean(), episode_length)\n",
        "    if i % 250 == 0:\n",
        "        print(\"Epoch {}, Loss: {}\".format(i,loss))\n",
        "        print(\"Forward loss: {} \\n Inverse loss: {} \\n Qloss: {}\".format(\\\n",
        "                             forward_pred_err.mean(),inverse_pred_err.mean(),q_loss.mean()))\n",
        "        print(info)\n",
        "    losses.append(loss_list)\n",
        "    loss.backward()\n",
        "    opt.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MncAkwJ5FPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses_ = np.array(losses)\n",
        "ep_lengths_ = np.array(ep_lengths)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(np.log(losses_[:,0]),label='Q loss')\n",
        "plt.plot(np.log(losses_[:,1]),label='Forward loss')\n",
        "plt.plot(np.log(losses_[:,2]),label='Inverse loss')\n",
        "#plt.plot(ep_lengths_, label='Episode Length')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4kAQ9nP5Hhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test model\n",
        "eps=0.15\n",
        "done = True\n",
        "state_deque = deque(maxlen=params['frames_per_state'])\n",
        "for step in range(5000):\n",
        "    if done:\n",
        "        env.reset()\n",
        "        state1 = prepare_initial_state(env.render('rgb_array'))\n",
        "    q_val_pred = Qmodel(state1)\n",
        "    action = int(policy(q_val_pred,eps))\n",
        "    state2, reward, done, info = env.step(action)\n",
        "    state2 = prepare_multi_state(state1,state2)\n",
        "    state1=state2\n",
        "    env.render()\n",
        "#env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gikTfVu5JVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}